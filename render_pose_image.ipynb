{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import yaml\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import Any, Dict, List, Literal, Optional, Union, Tuple\n",
    "from pathlib import Path\n",
    "import mediapy as media\n",
    "from contextlib import ExitStack\n",
    "from torch import Tensor\n",
    "\n",
    "from rich.panel import Panel\n",
    "from rich import box, style\n",
    "from rich.table import Table\n",
    "from rich.progress import (\n",
    "    BarColumn,\n",
    "    Progress,\n",
    "    TaskProgressColumn,\n",
    "    TextColumn,\n",
    "    TimeElapsedColumn,\n",
    "    TimeRemainingColumn,\n",
    ")\n",
    "\n",
    "from nerfstudio.cameras.cameras import Cameras, CameraType\n",
    "from nerfstudio.viewer.server.utils import three_js_perspective_camera_focal_length\n",
    "from nerfstudio.utils.rich_utils import CONSOLE, ItersPerSecColumn\n",
    "from nerfstudio.model_components import renderers\n",
    "from nerfstudio.engine.trainer import TrainerConfig\n",
    "from nerfstudio.pipelines.base_pipeline import Pipeline\n",
    "from nerfstudio.utils import colormaps\n",
    "from nerfstudio.data.scene_box import SceneBox\n",
    "from nerfstudio.configs.method_configs import all_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_from_json(camera_path) -> Cameras:\n",
    "    \"\"\"Takes a camera path dictionary and returns a trajectory as a Camera instance.\n",
    "\n",
    "    Args:\n",
    "        camera_path: A dictionary of the camera path information coming from the viewer.\n",
    "\n",
    "    Returns:\n",
    "        A Cameras instance with the camera path.\n",
    "    \"\"\"\n",
    "\n",
    "    image_height = camera_path[\"render_height\"]\n",
    "    image_width = camera_path[\"render_width\"]\n",
    "\n",
    "    if \"camera_type\" not in camera_path:\n",
    "        camera_type = CameraType.PERSPECTIVE\n",
    "    elif camera_path[\"camera_type\"] == \"fisheye\":\n",
    "        camera_type = CameraType.FISHEYE\n",
    "    elif camera_path[\"camera_type\"] == \"equirectangular\":\n",
    "        camera_type = CameraType.EQUIRECTANGULAR\n",
    "    elif camera_path[\"camera_type\"].lower() == \"omnidirectional\":\n",
    "        camera_type = CameraType.OMNIDIRECTIONALSTEREO_L\n",
    "    else:\n",
    "        camera_type = CameraType.PERSPECTIVE\n",
    "\n",
    "    c2ws = []\n",
    "    fxs = []\n",
    "    fys = []\n",
    "    for camera in camera_path[\"camera_path\"]:\n",
    "        # pose\n",
    "        c2w = torch.tensor(camera[\"camera_to_world\"]).view(4, 4)[:3]\n",
    "        c2ws.append(c2w)\n",
    "        if (\n",
    "            camera_type == CameraType.EQUIRECTANGULAR\n",
    "            or camera_type == CameraType.OMNIDIRECTIONALSTEREO_L\n",
    "            or camera_type == CameraType.OMNIDIRECTIONALSTEREO_R\n",
    "        ):\n",
    "            fxs.append(image_width / 2)\n",
    "            fys.append(image_height)\n",
    "        else:\n",
    "            # field of view\n",
    "            fov = camera[\"fov\"]\n",
    "            focal_length = three_js_perspective_camera_focal_length(fov, image_height)\n",
    "            fxs.append(focal_length)\n",
    "            fys.append(focal_length)\n",
    "\n",
    "    # Iff ALL cameras in the path have a \"time\" value, construct Cameras with times\n",
    "    if all(\"render_time\" in camera for camera in camera_path[\"camera_path\"]):\n",
    "        times = torch.tensor([camera[\"render_time\"] for camera in camera_path[\"camera_path\"]])\n",
    "    else:\n",
    "        times = None\n",
    "\n",
    "    camera_to_worlds = torch.stack(c2ws, dim=0)\n",
    "    fx = torch.tensor(fxs)\n",
    "    fy = torch.tensor(fys)\n",
    "    return Cameras(\n",
    "        fx=fx,\n",
    "        fy=fy,\n",
    "        cx=image_width / 2,\n",
    "        cy=image_height / 2,\n",
    "        camera_to_worlds=camera_to_worlds,\n",
    "        camera_type=camera_type,\n",
    "        times=times,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _render_trajectory_video(\n",
    "    pipeline: Pipeline,\n",
    "    cameras: Cameras,\n",
    "    output_filename: Path,\n",
    "    rendered_output_names: List[str],\n",
    "    crop_data = None,\n",
    "    rendered_resolution_scaling_factor: float = 1.0,\n",
    "    seconds: float = 5.0,\n",
    "    output_format: Literal[\"images\", \"video\"] = \"video\",\n",
    "    image_format: Literal[\"jpeg\", \"png\"] = \"jpeg\",\n",
    "    jpeg_quality: int = 100,\n",
    "    colormap_options: colormaps.ColormapOptions = colormaps.ColormapOptions(),\n",
    ") -> None:\n",
    "    \"\"\"Helper function to create a video of the spiral trajectory.\n",
    "\n",
    "    Args:\n",
    "        pipeline: Pipeline to evaluate with.\n",
    "        cameras: Cameras to render.\n",
    "        output_filename: Name of the output file.\n",
    "        rendered_output_names: List of outputs to visualise.\n",
    "        crop_data: Crop data to apply to the rendered images.\n",
    "        rendered_resolution_scaling_factor: Scaling factor to apply to the camera image resolution.\n",
    "        seconds: Length of output video.\n",
    "        output_format: How to save output data.\n",
    "        colormap_options: Options for colormap.\n",
    "    \"\"\"\n",
    "    CONSOLE.print(\"[bold green]Creating trajectory \" + output_format)\n",
    "    cameras.rescale_output_resolution(rendered_resolution_scaling_factor)\n",
    "    cameras = cameras.to(pipeline.device)\n",
    "    fps = len(cameras) / seconds\n",
    "\n",
    "    progress = Progress(\n",
    "        TextColumn(\":movie_camera: Rendering :movie_camera:\"),\n",
    "        BarColumn(),\n",
    "        TaskProgressColumn(\n",
    "            text_format=\"[progress.percentage]{task.completed}/{task.total:>.0f}({task.percentage:>3.1f}%)\",\n",
    "            show_speed=True,\n",
    "        ),\n",
    "        ItersPerSecColumn(suffix=\"fps\"),\n",
    "        TimeRemainingColumn(elapsed_when_finished=False, compact=False),\n",
    "        TimeElapsedColumn(),\n",
    "    )\n",
    "    output_image_dir = output_filename.parent / output_filename.stem\n",
    "    if output_format == \"images\":\n",
    "        output_image_dir.mkdir(parents=True, exist_ok=True)\n",
    "    if output_format == \"video\":\n",
    "        # make the folder if it doesn't exist\n",
    "        output_filename.parent.mkdir(parents=True, exist_ok=True)\n",
    "        # NOTE:\n",
    "        # we could use ffmpeg_args \"-movflags faststart\" for progressive download,\n",
    "        # which would force moov atom into known position before mdat,\n",
    "        # but then we would have to move all of mdat to insert metadata atom\n",
    "        # (unless we reserve enough space to overwrite with our uuid tag,\n",
    "        # but we don't know how big the video file will be, so it's not certain!)\n",
    "\n",
    "    with ExitStack() as stack:\n",
    "        writer = None\n",
    "\n",
    "        with progress:\n",
    "            for camera_idx in progress.track(range(cameras.size), description=\"\"):\n",
    "                aabb_box = None\n",
    "                if crop_data is not None:\n",
    "                    bounding_box_min = crop_data.center - crop_data.scale / 2.0\n",
    "                    bounding_box_max = crop_data.center + crop_data.scale / 2.0\n",
    "                    aabb_box = SceneBox(torch.stack([bounding_box_min, bounding_box_max]).to(pipeline.device))\n",
    "                camera_ray_bundle = cameras.generate_rays(camera_indices=camera_idx, aabb_box=aabb_box)\n",
    "                # print('camera_idx', camera_idx) [0, 1, ...]\n",
    "                # print('camera_ray_bundle.shape', camera_ray_bundle.shape)\n",
    "                # camera_ray_bundle.shape torch.Size([1080, 1920])\n",
    "\n",
    "                if crop_data is not None:\n",
    "                    with renderers.background_color_override_context(\n",
    "                        crop_data.background_color.to(pipeline.device)\n",
    "                    ), torch.no_grad():\n",
    "                        outputs = pipeline.model.get_outputs_for_camera_ray_bundle(camera_ray_bundle)\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        outputs = pipeline.model.get_outputs_for_camera_ray_bundle(camera_ray_bundle)\n",
    "\n",
    "                for output_str in outputs:\n",
    "                    print('output.shape', outputs[output_str].shape, 'output_str', output_str)\n",
    "\n",
    "                render_image = []\n",
    "                print('rendered_output_names', rendered_output_names)\n",
    "                for rendered_output_name in rendered_output_names:\n",
    "                    if rendered_output_name not in outputs:\n",
    "                        CONSOLE.rule(\"Error\", style=\"red\")\n",
    "                        CONSOLE.print(f\"Could not find {rendered_output_name} in the model outputs\", justify=\"center\")\n",
    "                        CONSOLE.print(\n",
    "                            f\"Please set --rendered_output_name to one of: {outputs.keys()}\", justify=\"center\"\n",
    "                        )\n",
    "                        # sys.exit(1)\n",
    "                    output_image = outputs[rendered_output_name]\n",
    "                    output_image = (\n",
    "                        colormaps.apply_colormap(\n",
    "                            image=output_image,\n",
    "                            colormap_options=colormap_options,\n",
    "                        )\n",
    "                        .cpu()\n",
    "                        .numpy()\n",
    "                    )\n",
    "                    render_image.append(output_image)\n",
    "                render_image = np.concatenate(render_image, axis=1)\n",
    "                print('np.shape(render_image)', np.shape(render_image))\n",
    "                if output_format == \"images\":\n",
    "                    if image_format == \"png\":\n",
    "                        media.write_image(output_image_dir / f\"{camera_idx:05d}.png\", render_image, fmt=\"png\")\n",
    "                    if image_format == \"jpeg\":\n",
    "                        media.write_image(\n",
    "                            output_image_dir / f\"{camera_idx:05d}.jpg\", render_image, fmt=\"jpeg\", quality=jpeg_quality\n",
    "                        )\n",
    "                if output_format == \"video\":\n",
    "                    if writer is None:\n",
    "                        render_width = int(render_image.shape[1])\n",
    "                        render_height = int(render_image.shape[0])\n",
    "                        writer = stack.enter_context(\n",
    "                            media.VideoWriter(\n",
    "                                path=output_filename,\n",
    "                                shape=(render_height, render_width),\n",
    "                                fps=fps,\n",
    "                            )\n",
    "                        )\n",
    "                    writer.add_image(render_image)\n",
    "\n",
    "    table = Table(\n",
    "        title=None,\n",
    "        show_header=False,\n",
    "        box=box.MINIMAL,\n",
    "        title_style=style.Style(bold=True),\n",
    "    )\n",
    "    if output_format == \"video\":\n",
    "        if cameras.camera_type[0] == CameraType.EQUIRECTANGULAR.value:\n",
    "            CONSOLE.print(\"Adding spherical camera data\")\n",
    "            # insert_spherical_metadata_into_file(output_filename)\n",
    "        table.add_row(\"Video\", str(output_filename))\n",
    "    else:\n",
    "        table.add_row(\"Images\", str(output_image_dir))\n",
    "    CONSOLE.print(Panel(table, title=\"[bold][green]:tada: Render Complete :tada:[/bold]\", expand=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_load_checkpoint(config: TrainerConfig, pipeline: Pipeline) -> Tuple[Path, int]:\n",
    "    ## TODO: ideally eventually want to get this to be the same as whatever is used to load train checkpoint too\n",
    "    \"\"\"Helper function to load checkpointed pipeline\n",
    "\n",
    "    Args:\n",
    "        config (DictConfig): Configuration of pipeline to load\n",
    "        pipeline (Pipeline): Pipeline instance of which to load weights\n",
    "    Returns:\n",
    "        A tuple of the path to the loaded checkpoint and the step at which it was saved.\n",
    "    \"\"\"\n",
    "    assert config.load_dir is not None\n",
    "    if config.load_step is None:\n",
    "        CONSOLE.print(\"Loading latest checkpoint from load_dir\")\n",
    "        # NOTE: this is specific to the checkpoint name format\n",
    "        if not os.path.exists(config.load_dir):\n",
    "            CONSOLE.rule(\"Error\", style=\"red\")\n",
    "            CONSOLE.print(f\"No checkpoint directory found at {config.load_dir}, \", justify=\"center\")\n",
    "            CONSOLE.print(\n",
    "                \"Please make sure the checkpoint exists, they should be generated periodically during training\",\n",
    "                justify=\"center\",\n",
    "            )\n",
    "            # sys.exit(1)\n",
    "        load_step = sorted(int(x[x.find(\"-\") + 1 : x.find(\".\")]) for x in os.listdir(config.load_dir))[-1]\n",
    "    else:\n",
    "        load_step = config.load_step\n",
    "    load_path = config.load_dir / f\"step-{load_step:09d}.ckpt\"\n",
    "    assert load_path.exists(), f\"Checkpoint {load_path} does not exist\"\n",
    "    loaded_state = torch.load(load_path, map_location=\"cpu\")\n",
    "    pipeline.load_pipeline(loaded_state[\"pipeline\"], loaded_state[\"step\"])\n",
    "    CONSOLE.print(f\":white_check_mark: Done loading checkpoint from {load_path}\")\n",
    "    return load_path, load_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tianshu/anaconda3/envs/nerfstudio/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:61: FutureWarning: Importing `PeakSignalNoiseRatio` from `torchmetrics` was deprecated and will be removed in 2.0. Import `PeakSignalNoiseRatio` from `torchmetrics.image` instead.\n",
      "  _future_warning(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading latest checkpoint from load_dir\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading latest checkpoint from load_dir\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅ Done loading checkpoint from outputs/poster/nerfacto/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">07</span>-16_144731/nerfstudio_models/step-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000006000.</span>ckpt\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅ Done loading checkpoint from outputs/poster/nerfacto/\u001b[1;36m2023\u001b[0m-\u001b[1;36m07\u001b[0m-16_144731/nerfstudio_models/step-\u001b[1;36m000006000.\u001b[0mckpt\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config_path = Path('outputs/poster/nerfacto/2023-07-16_144731/config.yml')\n",
    "config = yaml.load(config_path.read_text(), Loader=yaml.Loader)\n",
    "config.pipeline.datamanager._target = all_methods[config.method_name].pipeline.datamanager._target\n",
    "config.load_dir = config.get_checkpoint_dir()\n",
    "config.pipeline.datamanager.eval_image_indices = None\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pipeline = config.pipeline.setup(device=device, test_mode='inference')\n",
    "pipeline.eval()\n",
    "checkpoint_path, step = eval_load_checkpoint(config, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Creating trajectory images</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mCreating trajectory images\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6782fe1cd83346bda99cdb928dc9a78b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">output.shape torch.Size([1080, 1920, 3]) output_str rgb\n",
       "</pre>\n"
      ],
      "text/plain": [
       "output.shape torch.Size([1080, 1920, 3]) output_str rgb\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">output.shape torch.Size([1080, 1920, 1]) output_str accumulation\n",
       "</pre>\n"
      ],
      "text/plain": [
       "output.shape torch.Size([1080, 1920, 1]) output_str accumulation\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">output.shape torch.Size([1080, 1920, 1]) output_str depth\n",
       "</pre>\n"
      ],
      "text/plain": [
       "output.shape torch.Size([1080, 1920, 1]) output_str depth\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">output.shape torch.Size([1080, 1920, 1]) output_str prop_depth_0\n",
       "</pre>\n"
      ],
      "text/plain": [
       "output.shape torch.Size([1080, 1920, 1]) output_str prop_depth_0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">output.shape torch.Size([1080, 1920, 1]) output_str prop_depth_1\n",
       "</pre>\n"
      ],
      "text/plain": [
       "output.shape torch.Size([1080, 1920, 1]) output_str prop_depth_1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">rendered_output_names ['rgb']\n",
       "</pre>\n"
      ],
      "text/plain": [
       "rendered_output_names ['rgb']\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">np.shape(render_image) (1080, 1920, 3)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "np.shape(render_image) (1080, 1920, 3)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">output.shape torch.Size([1080, 1920, 3]) output_str rgb\n",
       "</pre>\n"
      ],
      "text/plain": [
       "output.shape torch.Size([1080, 1920, 3]) output_str rgb\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">output.shape torch.Size([1080, 1920, 1]) output_str accumulation\n",
       "</pre>\n"
      ],
      "text/plain": [
       "output.shape torch.Size([1080, 1920, 1]) output_str accumulation\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">output.shape torch.Size([1080, 1920, 1]) output_str depth\n",
       "</pre>\n"
      ],
      "text/plain": [
       "output.shape torch.Size([1080, 1920, 1]) output_str depth\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">output.shape torch.Size([1080, 1920, 1]) output_str prop_depth_0\n",
       "</pre>\n"
      ],
      "text/plain": [
       "output.shape torch.Size([1080, 1920, 1]) output_str prop_depth_0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">output.shape torch.Size([1080, 1920, 1]) output_str prop_depth_1\n",
       "</pre>\n"
      ],
      "text/plain": [
       "output.shape torch.Size([1080, 1920, 1]) output_str prop_depth_1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">rendered_output_names ['rgb']\n",
       "</pre>\n"
      ],
      "text/plain": [
       "rendered_output_names ['rgb']\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">np.shape(render_image) (1080, 1920, 3)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "np.shape(render_image) (1080, 1920, 3)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">output.shape torch.Size([1080, 1920, 3]) output_str rgb\n",
       "</pre>\n"
      ],
      "text/plain": [
       "output.shape torch.Size([1080, 1920, 3]) output_str rgb\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">output.shape torch.Size([1080, 1920, 1]) output_str accumulation\n",
       "</pre>\n"
      ],
      "text/plain": [
       "output.shape torch.Size([1080, 1920, 1]) output_str accumulation\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">output.shape torch.Size([1080, 1920, 1]) output_str depth\n",
       "</pre>\n"
      ],
      "text/plain": [
       "output.shape torch.Size([1080, 1920, 1]) output_str depth\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">output.shape torch.Size([1080, 1920, 1]) output_str prop_depth_0\n",
       "</pre>\n"
      ],
      "text/plain": [
       "output.shape torch.Size([1080, 1920, 1]) output_str prop_depth_0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">output.shape torch.Size([1080, 1920, 1]) output_str prop_depth_1\n",
       "</pre>\n"
      ],
      "text/plain": [
       "output.shape torch.Size([1080, 1920, 1]) output_str prop_depth_1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">rendered_output_names ['rgb']\n",
       "</pre>\n"
      ],
      "text/plain": [
       "rendered_output_names ['rgb']\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">np.shape(render_image) (1080, 1920, 3)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "np.shape(render_image) (1080, 1920, 3)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">output.shape torch.Size([1080, 1920, 3]) output_str rgb\n",
       "</pre>\n"
      ],
      "text/plain": [
       "output.shape torch.Size([1080, 1920, 3]) output_str rgb\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">output.shape torch.Size([1080, 1920, 1]) output_str accumulation\n",
       "</pre>\n"
      ],
      "text/plain": [
       "output.shape torch.Size([1080, 1920, 1]) output_str accumulation\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">output.shape torch.Size([1080, 1920, 1]) output_str depth\n",
       "</pre>\n"
      ],
      "text/plain": [
       "output.shape torch.Size([1080, 1920, 1]) output_str depth\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">output.shape torch.Size([1080, 1920, 1]) output_str prop_depth_0\n",
       "</pre>\n"
      ],
      "text/plain": [
       "output.shape torch.Size([1080, 1920, 1]) output_str prop_depth_0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">output.shape torch.Size([1080, 1920, 1]) output_str prop_depth_1\n",
       "</pre>\n"
      ],
      "text/plain": [
       "output.shape torch.Size([1080, 1920, 1]) output_str prop_depth_1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">rendered_output_names ['rgb']\n",
       "</pre>\n"
      ],
      "text/plain": [
       "rendered_output_names ['rgb']\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">np.shape(render_image) (1080, 1920, 3)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "np.shape(render_image) (1080, 1920, 3)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭────── <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">🎉 Render Complete 🎉</span> ───────╮\n",
       "│          ╷                         │\n",
       "│   Images │ Jupyter_output/output   │\n",
       "│          ╵                         │\n",
       "╰────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭────── \u001b[1;32m🎉 Render Complete 🎉\u001b[0m ───────╮\n",
       "│          ╷                         │\n",
       "│   Images │ Jupyter_output/output   │\n",
       "│          ╵                         │\n",
       "╰────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_path = Path('Jupyter_output/output.mp4')\n",
    "camera_path_filename = 'data/nerfstudio/poster/camera_paths/test.json'\n",
    "colormap_options = colormaps.ColormapOptions()\n",
    "with open(camera_path_filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    camera_path = json.load(f)\n",
    "camera_path = get_path_from_json(camera_path)\n",
    "_render_trajectory_video(\n",
    "            pipeline,\n",
    "            camera_path,\n",
    "            output_filename=output_path,\n",
    "            rendered_output_names=['rgb'],\n",
    "            rendered_resolution_scaling_factor=1.0,\n",
    "            output_format='images',\n",
    "            image_format='jpeg',\n",
    "            jpeg_quality=100,\n",
    "            colormap_options=colormap_options,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerfstudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
